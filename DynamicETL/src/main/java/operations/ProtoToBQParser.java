package operations;

import com.google.api.services.bigquery.model.*;
import lvi.BQParserImp;
import org.apache.beam.sdk.io.gcp.bigquery.*;
import org.apache.beam.sdk.transforms.*;
import org.apache.beam.sdk.values.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * {@link PTransform} to write Protocol buffer events to their destination
 * BigQuery table as defined in the Proto file.
 *
 * NOTE: Requires code files generated by the ProtoToBQ Proto compiler plugin.
 */
public class ProtoToBQParser<InputT> extends PTransform<PCollection<InputT>, PDone> {

    private static final Logger LOG = LoggerFactory.getLogger(ProtoToBQParser.class);

    private final String project;
    private final SerializableFunction<InputT, String> protoTypeExtractor;
    private final SerializableFunction<InputT, byte[]> protoPayloadExtractor;
    private final SerializableFunction<InputT, String> datasetExtractor;

    public ProtoToBQParser(String project,
                           SerializableFunction<InputT, String> protoTypeExtractor,
                           SerializableFunction<InputT, String> datasetExtractor,
                           SerializableFunction<InputT, byte[]> protoPayloadExtractor) {

        this.project = project;
        this.protoTypeExtractor = protoTypeExtractor;
        this.datasetExtractor = datasetExtractor;
        this.protoPayloadExtractor = protoPayloadExtractor;
    }

    @Override
    public PDone expand(PCollection<InputT> input) {

        // Apply ParDo
        input
                .apply("ProtoToBQ", ParDo.of(new ProtoToBQ()))
                .apply(BigQueryIO.<KV<KV<TableDestination, String>, TableRow>>write()
                        .to(new ProtoToBQDynamicDestinations())
                        .withFormatFunction(new ExtractValues<>())
                        .withFailedInsertRetryPolicy(InsertRetryPolicy.retryTransientErrors())
                        .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_EMPTY)
                        .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED));

        return PDone.in(input.getPipeline());
    }

    /**
     * {@link SerializableFunction} to derive a stream of values from a KV stream.
     *
     * @param <KeyT> type of the keys
     * @param <ValueT> type of the values
     */
    private static class ExtractValues<KeyT, ValueT> implements SerializableFunction<KV<KeyT, ValueT>, ValueT> {

        @Override
        public ValueT apply(KV<KeyT, ValueT> input) {
            return input.getValue();
        }
    }

    /**
     * {@link DoFn} to augment input event with routing information for the ProtoToBQ plugin.
     */
    private class ProtoToBQ extends DoFn<InputT, KV<KV<TableDestination, String>, TableRow>> {

        @ProcessElement
        public void processElement(@Element InputT input, ProcessContext c) {

            try {

                // Retrieve parser for type
                String protoType = protoTypeExtractor.apply(input);
                BQParserImp parser = BQParserImp.getParserForType(protoType);

                // Extract rows and destination
                for(TableRow row: parser.convertToTableRow(protoPayloadExtractor.apply(input))) {

                    LOG.info(row.toPrettyString());
                    LOG.info(constructTableRef(project, datasetExtractor.apply(input), parser.getBigQueryTableName()));

                    // Generate output object
                    c.output(KV.of(
                            KV.of(new TableDestination(constructTableRef(project, datasetExtractor.apply(input), parser.getBigQueryTableName()),
                                            parser.getBigQueryTableDescription(),
                                            parser.getPartitioning(),
                                            parser.getClustering()),
                                    protoType),
                            row));
                }
            } catch (Exception e) {
                LOG.info("Error " + e.getMessage());

                // FUTURE: Add DLQ path to pipeline
                // c.output(DLQTag, new FailSafeElement<>(input, e.getMessage()));
            }
        }

        private String constructTableRef(String project, String datasetName, String tableName) {
            return project + ":" + datasetName + "." + tableName;
        }
    }

    /**
     * {@link DynamicDestinations} to route the input events to the
     * appropriate table as defined by the ProtoToBQ plugin.
     */
    private class ProtoToBQDynamicDestinations extends DynamicDestinations<KV<KV<TableDestination, String>, TableRow>, KV<TableDestination, String>> {

        @Override
        public KV<TableDestination, String> getDestination(ValueInSingleWindow<KV<KV<TableDestination, String>, TableRow>> element) {
            return element.getValue().getKey();
        }

        @Override
        public TableDestination getTable(KV<TableDestination, String> destination) {
            return destination.getKey();
        }

        @Override
        public TableSchema getSchema(KV<TableDestination, String> destination) {
            try {
                // TableSchemas are not serializable by default, hence we obtain it here
                return BQParserImp.getParserForType(destination.getValue()).getBigQueryTableSchema();
            } catch (Exception e) {
                throw new RuntimeException();
            }
        }
    }
}
