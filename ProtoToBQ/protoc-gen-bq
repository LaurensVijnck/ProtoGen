#!/usr/bin/env python3
import itertools
import io
import sys
import logging

from enum import Enum
from pathlib import Path
from jsondiff import diff

from output.python.protos import bigquery_options_pb2
from google.cloud import bigquery
from google.protobuf.compiler import plugin_pb2 as plugin
from google.protobuf.descriptor_pb2 import DescriptorProto, EnumDescriptorProto

from codegen import *
from enums import ProtoTypeEnum

_PROTO_TO_JAVA_TYPE_MAP = {
    ProtoTypeEnum.TYPE_DOUBLE: "Double",
    ProtoTypeEnum.TYPE_FLOAT: "Float",
    ProtoTypeEnum.TYPE_ENUM: "String",  # FUTURE: Map onto int value?
    ProtoTypeEnum.TYPE_INT64: "Integer",
    ProtoTypeEnum.TYPE_SINT64: "Long",
    ProtoTypeEnum.TYPE_SFIXED64: "Long",
    ProtoTypeEnum.TYPE_UINT64: "Long",
    ProtoTypeEnum.TYPE_FIXED64: "Long",
    ProtoTypeEnum.TYPE_INT32: "Integer",
    ProtoTypeEnum.TYPE_SFIXED32: "Integer",
    ProtoTypeEnum.TYPE_SINT32: "Integer",
    ProtoTypeEnum.TYPE_UINT32: "Integer",
    ProtoTypeEnum.TYPE_FIXED32: "Integer",
    ProtoTypeEnum.TYPE_BYTES: "Bytes",
    ProtoTypeEnum.TYPE_STRING: "String",
    ProtoTypeEnum.TYPE_BOOL: "Boolean",
    ProtoTypeEnum.TYPE_MESSAGE: None,
    ProtoTypeEnum.TYPE_GROUP: None
}

_PROTO_TO_BQ_TYPE_MAP = {
    ProtoTypeEnum.TYPE_DOUBLE: "NUMERIC",
    ProtoTypeEnum.TYPE_FLOAT: "FLOAT64",
    ProtoTypeEnum.TYPE_ENUM: "STRING",  # FUTURE: Map onto int value?
    ProtoTypeEnum.TYPE_INT64: "INTEGER",
    ProtoTypeEnum.TYPE_SINT64: "INTEGER",
    ProtoTypeEnum.TYPE_SFIXED64: "INTEGER",
    ProtoTypeEnum.TYPE_UINT64: "INTEGER",
    ProtoTypeEnum.TYPE_FIXED64: "INTEGER",
    ProtoTypeEnum.TYPE_INT32: "INTEGER",
    ProtoTypeEnum.TYPE_SFIXED32: "INTEGER",
    ProtoTypeEnum.TYPE_SINT32: "INTEGER",
    ProtoTypeEnum.TYPE_UINT32: "INTEGER",
    ProtoTypeEnum.TYPE_FIXED32: "INTEGER",
    ProtoTypeEnum.TYPE_BYTES: "BYTES",
    ProtoTypeEnum.TYPE_STRING: "STRING",
    ProtoTypeEnum.TYPE_BOOL: "BOOL",
    ProtoTypeEnum.TYPE_MESSAGE: "RECORD",
    ProtoTypeEnum.TYPE_GROUP: "RECORD"
}


# https://expobrain.net/2015/09/13/create-a-plugin-for-google-protocol-buffer/
def _traverse(proto_file):
    """
    Function to return all the items in the given proto file.

    :param proto_file:
    :return: list containing all the items in the proto file.
    """
    def _traverse(package, items):
        for item in items:
            yield item, package

            if isinstance(item, DescriptorProto):
                for enum in item.enum_type:
                    yield enum, package

                for nested in item.nested_type:
                    nested_package = package + item.name

                    for nested_item in _traverse(nested, nested_package):
                        yield nested_item, nested_package

    return itertools.chain(
        _traverse(proto_file.package, proto_file.enum_type),
        _traverse(proto_file.package, proto_file.message_type),
    )


def _resolve_repository_fields(request: plugin.CodeGeneratorRequest, repository):
    """
    Function to resolve the fields of the items in the repository.

    :param request:
    :param repository:
    :return:
    """
    for proto_file in request.proto_file:

        # Parse request
        for item, package in _traverse(proto_file):

            if isinstance(item, DescriptorProto):

                field_type = repository.get(FieldType.to_fq_name(proto_file.package, item.name))
                fields = []
                batch_table = False

                for f in item.field:

                    default_value = None

                    if f.options.HasExtension(bigquery_options_pb2.default_value):
                        default_extension = f.options.Extensions[bigquery_options_pb2.default_value]
                        default_value = getattr(default_extension, default_extension.WhichOneof('value_oneof'))

                    # TODO: make sure we respect proto indexes for ordering (to avoid diff issues)
                    fields.append(
                        Field(
                            field_index=f.number,
                            field_name=f.name,
                            field_alias=f.options.Extensions[bigquery_options_pb2.alias] if f.options.HasExtension(bigquery_options_pb2.alias) else None,
                            field_description=f.options.Extensions[bigquery_options_pb2.description],
                            field_type=ProtoTypeEnum._member_map_[ProtoTypeEnum(f.type).name],
                            field_type_value=repository.get(f.type_name, None),
                            field_required=f.options.Extensions[bigquery_options_pb2.required],
                            is_batch_field=f.options.Extensions[bigquery_options_pb2.batch_attribute],
                            is_clustering_field=f.options.Extensions[bigquery_options_pb2.clustering_attribute],
                            is_timestamp=f.options.Extensions[bigquery_options_pb2.timestamp_attribute],
                            is_partitioning_field=f.options.Extensions[bigquery_options_pb2.timestamp_attribute],
                            is_optional_field=f.proto3_optional,
                            is_repeated_field=f.label == f.LABEL_REPEATED,
                            default_value=default_value
                        )
                    )

                    batch_table = batch_table or f.options.Extensions[bigquery_options_pb2.batch_attribute]

                    # FUTURE: Check that there is solely a single partition attribute
                    # FUTURE: Check that the partition field is defined at top-level
                    # FUTURE: Refactor alias handling
                    if f.options.Extensions[bigquery_options_pb2.time_partitioning_attribute]:
                        field_type.set_partition_field(f.options.Extensions[bigquery_options_pb2.alias] if f.options.HasExtension(bigquery_options_pb2.alias) else f.name)

                    # FUTURE: Check that there is solely a single cluster attribute
                    # FUTURE: Check that the cluster field is defined at top-level
                    # FUTURE: Refactor alias handling
                    if f.options.Extensions[bigquery_options_pb2.clustering_attribute]:
                        field_type.add_cluster_field(f.options.Extensions[bigquery_options_pb2.alias] if f.options.HasExtension(bigquery_options_pb2.alias) else f.name)

                # FUTURE: Maybe not the ideal position to set this.
                field_type.set_batch_table(batch_table)
                field_type.set_fields(fields)


def _generate_repository(request: plugin.CodeGeneratorRequest):
    """
    Function to generate a repository from the given request.

    :param request:
    :return: Repository
    """
    repository = {}
    root_elements = []

    # Make sure all types are present
    for proto_file in request.proto_file:

        ambiguous_file_name = False

        # Check whether out class should be deduplicated
        for item, package in _traverse(proto_file):
            if isinstance(item, DescriptorProto) or isinstance(item, EnumDescriptorProto):
                ambiguous_file_name = ambiguous_file_name or Path(proto_file.name).stem.lower() == item.name.lower()

        # Parse request
        for item, package in _traverse(proto_file):

            if isinstance(item, DescriptorProto) or isinstance(item, EnumDescriptorProto):

                field_type = None

                if isinstance(item, DescriptorProto):

                    table_root = item.options.HasExtension(bigquery_options_pb2.bq_root_options)
                    field_type = MessageFieldType(proto_file.package, Path(proto_file.name).stem, item.name, ambiguous_file_name)

                    if table_root:
                        # Extract root options
                        root_options = item.options.Extensions[bigquery_options_pb2.bq_root_options]
                        field_type.set_table_root(True, root_options.table_name, root_options.table_description, root_options.time_partitioning, root_options.time_partitioning_expiration_ms)
                        root_elements.append(field_type)

                elif isinstance(item, EnumDescriptorProto):

                    field_type = EnumFieldType(proto_file.package, Path(proto_file.name).stem, item.name, ambiguous_file_name)
                    field_type.set_values([EnumFieldValue(v.name, v.number) for v in item.value])

                repository[field_type.get_fq_name()] = field_type

    _resolve_repository_fields(request, repository)

    return repository, root_elements


def _contruct_bigquery_schema_rec(field: MessageFieldType, schema_fields: list):
    """
    Function to recursively generate a BigQuery schema for a message.

    FUTURE: When supporting multi-language codegen, the CodeGenSchema node
    should handle the generation of the JSON version of the schema.

    :param field: MessageFieldType
    :param schema_fields: wrapper for the fields
    :return:
    """
    if field is None:
        return []

    for field in field.fields:

        if field.is_batch_field:
            schema_fields.extend(_contruct_bigquery_schema_rec(field.field_type_value, []))

        else:
            schema_fields.append(
                bigquery.SchemaField(
                    name=field.get_bigquery_field_name(),
                    description=field.field_description,
                    mode="REPEATED" if field.is_repeated_field else "REQUIRED" if field.field_required else "NULLABLE",
                    field_type=_PROTO_TO_BQ_TYPE_MAP[field.field_type] if not field.is_timestamp else "TIMESTAMP",
                    fields=_contruct_bigquery_schema_rec(field.field_type_value, [])
                )
            )

    return schema_fields


def _add_codegen_node_conditionally(root: CodeGenImp, node: CodeGenImp, condition = True):
    """
    Function to conditionally add a node to the tree, if the conditional succeeds, the node is
    added and is returned by the function. If the conditional fails, the old root is returned.

    :param root:
    :param node:
    :param condition:
    :return:
    """
    if condition:
        root.add_child(node)
        return node

    return root


def codegen_rec(field_type: MessageFieldType, root: CodeGenImp, table_root: bool = False):
    """
    Recursive function to build the code generation tee.

    :param field_type:
    :param root:
    :param table_root:
    :return:
    """
    node = CodeNopNode(table_root, field_type.batch_table)
    for field in field_type.fields:

        field_root = node
        field_root = _add_codegen_node_conditionally(field_root, CodeGenRepeatedNode(field), field.is_repeated_field and not (table_root and field.is_batch_field))

        if field.field_type == ProtoTypeEnum.TYPE_MESSAGE:
            if table_root and field.is_batch_field:
                # Batch field attached directly to the root, as to allow
                # grouping of the batch and non-batch nodes.
                field_root = _add_codegen_node_conditionally(root, CodeGenGetBatchNode(field, node))
            else:
                field_root = _add_codegen_node_conditionally(field_root, CodeGenNestedNode(field), not field.is_repeated_field)
                field_root = _add_codegen_node_conditionally(field_root, CodeGenConditionalNode(field, field.field_required, field.default_value), not field.is_repeated_field)
                field_root = _add_codegen_node_conditionally(field_root, CodeGenGetFieldNode(field), not field.is_repeated_field)

            # Recurse child fields
            codegen_rec(field.field_type_value, field_root)
        else:
            field_root = _add_codegen_node_conditionally(field_root, CodeGenConditionalNode(field, field.field_required, field.default_value), field.is_optional_field or field.default_value is not None)
            field_root = _add_codegen_node_conditionally(field_root, CodeGenGetFieldNode(field), not field.is_repeated_field)
            field_root.add_child(CodeGenBaseNode(field))

    root.add_child(node)
    return root


def create_codegen_tree(class_name: str, interface_name: str, root: MessageFieldType, function_name: str):
    """
    Create the code generation tee for the given MessageField.

    :param class_name
    :param root:
    :param interfaces:
    :return:
    """
    class_node = CodeGenClassNode(class_name, interface_name, root)

    function_node = CodeGenFunctionNode(function_name, root)
    schema_node = CodeGenSchemaFunctionNode(root, _PROTO_TO_BQ_TYPE_MAP)
    class_node.add_child(schema_node)
    class_node.add_child(function_node)

    codegen_rec(root, function_node, table_root=True)

    return class_node


def read_repository(repository_path: str) -> dict:
    """
    Read repository from file.

    :param repository_path: (str) Repository path.
    :return: (dict) Repository
    """
    with open(repository_path) as fp:
        repo = json.loads(fp.read())

    return repo


def validate_changes(new_repo: dict, old_repo: dict):
    """
    Function to detect conflicting changes in the new and
    old repositories.

    :param old_repo:
    :param new_repo:
    :return:
    """
    logging.warning(diff(old_repo, new_repo))


def generate_code(request: plugin.CodeGeneratorRequest, response: plugin.CodeGeneratorResponse):
    """
    :param request: the plugin's input source
    :param response: the plugin's output sink
    :return: None
    """

    client = bigquery.Client() # JSON Serializing the schema requires the BigQuery client :(
    repo, root_elements = _generate_repository(request)

    # FUTURE: Enforce that required and fields with defaults are marked optional.
    # FUTURE: Add validation to old and new repository to detect conflicting changes.
    if request.parameter:
        new_repo = {k: v.to_json() for k, v in repo.items()}
        old_repo = read_repository(request.parameter)
        validate_changes(new_repo, old_repo)

    package_name = "lvi"
    interface_name = "BQParserImp"
    function_name = "convertToTableRow"
    parsers = {}

    for root_element in root_elements:
        schema_fields = []
        _contruct_bigquery_schema_rec(root_element, schema_fields)

        # Generate the BigQuery schema
        f = io.StringIO("")
        client.schema_to_json(schema_fields, f)
        full_schema = {}
        full_schema["schema_fields"] = json.loads(f.getvalue())
        full_schema["clustering_fields"] = root_element.cluster_fields
        full_schema["partitioning_field"] = root_element.partition_field
        full_schema["table_description"] = root_element.table_description
        full_schema["table_name"] = root_element.table_name

        file = response.file.add()
        file.name = "schema_" + root_element.name.lower() + ".json"
        file.content = json.dumps(full_schema, indent=2)

        # Generate the ProtoToBQParser
        file = response.file.add()
        class_name = root_element.name + "BQParser"
        file.name = class_name + ".java"
        create_codegen_tree(class_name, f'{package_name}.{interface_name}', root_element, function_name).gen_code(file, None, None, 0, _PROTO_TO_JAVA_TYPE_MAP)

        # Add to parsers
        parsers[root_element.get_fq_name()[1:]] = f'{package_name}.{class_name}'

    # Generate base class
    file = response.file.add()
    file.name = interface_name + ".java"
    interface_node = CodeGenInterfaceNode(package_name, "BQParserImp", function_name, parsers)  # TODO Where to fetch the package from?
    interface_node.gen_code(file, None, None, 0, _PROTO_TO_JAVA_TYPE_MAP)

    # Drop new repository
    f = response.file.add()
    f.name = 'repository.json'
    f.content = json.dumps({k: v.to_json() for k, v in repo.items()}, indent=2)


if __name__ == '__main__':

    # Read request message from stdin
    data = sys.stdin.buffer.read()

    # Parse request
    request = plugin.CodeGeneratorRequest()
    request.ParseFromString(data)

    # Create response
    response = plugin.CodeGeneratorResponse()
    response.supported_features = response.FEATURE_PROTO3_OPTIONAL

    # Generate code
    generate_code(request, response)

    # Serialise response message
    output = response.SerializeToString()

    # Write to stdout
    sys.stdout.buffer.write(output)
